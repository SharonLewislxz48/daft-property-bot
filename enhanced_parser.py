#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
"""
import asyncio
import aiohttp
import random
from typing import List, Optional
from bs4 import BeautifulSoup
import time
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedDaftParser:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
    
    def __init__(self):
        self.base_url = "https://www.daft.ie"
        self.session = None
        
        # –†–æ—Ç–∞—Ü–∏—è User-Agent
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0"
        ]
    
    async def get_session(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        if not self.session:
            # –°–ª—É—á–∞–π–Ω—ã–π User-Agent
            user_agent = random.choice(self.user_agents)
            
            headers = {
                'User-Agent': user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Å—Å–∏–∏
            connector = aiohttp.TCPConnector(
                limit=10,
                limit_per_host=2,
                ttl_dns_cache=300,
                use_dns_cache=True
            )
            
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            
            self.session = aiohttp.ClientSession(
                headers=headers,
                connector=connector,
                timeout=timeout,
                trust_env=True  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            )
        
        return self.session
    
    async def fetch_page(self, url: str, delay: float = None) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        session = await self.get_session()
        
        # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞
        if delay is None:
            delay = random.uniform(2.0, 5.0)
        
        logger.info(f"Fetching {url} with delay {delay:.1f}s")
        await asyncio.sleep(delay)
        
        for attempt in range(3):  # 3 –ø–æ–ø—ã—Ç–∫–∏
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ—Ñ—Ñ–µ—Ä–µ—Ä –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                headers = {}
                if 'page=' in url and 'page=1' not in url:
                    headers['Referer'] = self.base_url
                
                async with session.get(url, headers=headers) as response:
                    logger.info(f"Response status: {response.status}")
                    
                    if response.status == 200:
                        content = await response.text()
                        if len(content) > 1000:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                            logger.info(f"Successfully fetched page, content length: {len(content)}")
                            return content
                        else:
                            logger.warning(f"Received short content: {len(content)} bytes")
                    
                    elif response.status == 403:
                        logger.warning(f"Attempt {attempt + 1}: Access forbidden (403)")
                        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ
                        await asyncio.sleep(random.uniform(5.0, 10.0))
                    
                    elif response.status == 429:
                        logger.warning(f"Attempt {attempt + 1}: Rate limited (429)")
                        await asyncio.sleep(random.uniform(10.0, 20.0))
                    
                    else:
                        logger.warning(f"Attempt {attempt + 1}: Unexpected status {response.status}")
                        
            except Exception as e:
                logger.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt < 2:  # –ù–µ –∂–¥—ë–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–∏
                    await asyncio.sleep(random.uniform(3.0, 8.0))
        
        logger.error(f"Failed to fetch {url} after 3 attempts")
        return None
    
    async def parse_listings(self, html: str) -> List[dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        listings = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        selectors = [
            'div[data-testid="results"] > div',
            '.SearchPage__Results > div',
            '[data-testid="property-card"]',
            '.PropertySearchCard',
            'div[data-testid*="property"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                logger.info(f"Found {len(elements)} elements with selector: {selector}")
                
                for element in elements:
                    try:
                        listing = self.parse_single_listing(element)
                        if listing:
                            listings.append(listing)
                    except Exception as e:
                        logger.debug(f"Error parsing listing: {e}")
                        continue
                
                if listings:
                    break
        
        logger.info(f"Successfully parsed {len(listings)} listings")
        return listings
    
    def parse_single_listing(self, element) -> Optional[dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏
            link_elem = element.find('a', href=True)
            if not link_elem:
                return None
            
            url = link_elem['href']
            if not url.startswith('http'):
                url = self.base_url + url
            
            # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_selectors = ['h2', 'h3', '.title', '[data-testid*="title"]']
            title = None
            for selector in title_selectors:
                title_elem = element.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break
            
            if not title:
                return None
            
            # –ü–æ–∏—Å–∫ —Ü–µ–Ω—ã
            price_selectors = [
                '[data-testid*="price"]',
                '.price',
                '[class*="price" i]',
                'span:contains("‚Ç¨")'
            ]
            price = None
            for selector in price_selectors:
                price_elem = element.select_one(selector)
                if price_elem and '‚Ç¨' in price_elem.get_text():
                    price = price_elem.get_text(strip=True)
                    break
            
            # –ü–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞
            address_selectors = [
                '[data-testid*="address"]',
                '.address',
                '[class*="address" i]'
            ]
            address = None
            for selector in address_selectors:
                addr_elem = element.select_one(selector)
                if addr_elem:
                    address = addr_elem.get_text(strip=True)
                    break
            
            return {
                'title': title,
                'price': price or 'Price on request',
                'address': address or 'Address not specified',
                'url': url,
                'bedrooms': '2+ beds',  # –ó–∞–≥–ª—É—à–∫–∞
                'description': f"Property in {address or 'Dublin'}"
            }
            
        except Exception as e:
            logger.debug(f"Error parsing single listing: {e}")
            return None
    
    async def search_properties(self, city="Dublin", max_price=3000, min_bedrooms=2):
        """–ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        logger.info(f"Searching properties in {city}, max price: ‚Ç¨{max_price}, min bedrooms: {min_bedrooms}")
        
        # –°—Ç—Ä–æ–∏–º URL –ø–æ–∏—Å–∫–∞
        search_url = f"{self.base_url}/property-for-rent/{city.lower()}"
        params = []
        
        if max_price:
            params.append(f"rentalPrice_to={max_price}")
        if min_bedrooms:
            params.append(f"numBeds_from={min_bedrooms}")
        
        if params:
            search_url += "?" + "&".join(params)
        
        all_listings = []
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        for page in range(1, 4):  # –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_url = f"{search_url}&page={page}" if '?' in search_url else f"{search_url}?page={page}"
            
            html = await self.fetch_page(page_url)
            if html:
                listings = await self.parse_listings(html)
                all_listings.extend(listings)
                
                if not listings:  # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                    break
            else:
                logger.warning(f"Failed to fetch page {page}")
                break
        
        logger.info(f"Total listings found: {len(all_listings)}")
        return all_listings
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

# –¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
async def test_enhanced_parser():
    parser = EnhancedDaftParser()
    
    try:
        print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏...")
        print("=" * 60)
        
        listings = await parser.search_properties("Dublin", 3000, 2)
        
        if listings:
            print(f"‚úÖ –£–°–ü–ï–•! –ù–∞–π–¥–µ–Ω–æ {len(listings)} –†–ï–ê–õ–¨–ù–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
            print()
            
            for i, listing in enumerate(listings[:5], 1):
                print(f"   {i}. üè† {listing['title']}")
                print(f"      üìç {listing['address']}")
                print(f"      üí∞ {listing['price']}")
                print(f"      üîó {listing['url']}")
                print()
            
            return True
        else:
            print("‚ùå –û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False
    finally:
        await parser.close()

if __name__ == "__main__":
    success = asyncio.run(test_enhanced_parser())
    if success:
        print("üéâ –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç! –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
    else:
        print("‚ö†Ô∏è –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ç–æ–∂–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω.")
