#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä daft.ie —Å –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é:
- –û–±—Ö–æ–¥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–∞–ª–µ–Ω
- Retry –ª–æ–≥–∏–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import asyncio
import re
import json
import datetime
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from playwright.async_api import async_playwright
import time

class ProductionDaftParser:
    """–ü—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä daft.ie"""
    
    def __init__(self, log_level: str = "INFO"):
        self.base_url = "https://www.daft.ie"
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self._setup_logging(log_level)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_pages': 0,
            'total_links_found': 0,
            'total_processed': 0,
            'successful_parses': 0,
            'failed_parses': 0,
            'retries': 0,
            'start_time': None,
            'end_time': None
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.max_retries = 3
        self.retry_delay = 2
        self.page_timeout = 30000
        self.property_timeout = 15000
    
    def _setup_logging(self, level: str):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤
        Path("logs").mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=getattr(logging, level.upper()),
            format=log_format,
            handlers=[
                logging.FileHandler(f'logs/daft_parser_{datetime.datetime.now().strftime("%Y%m%d")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    async def search_all_properties(
        self, 
        min_bedrooms: int = 3, 
        max_price: int = 2500, 
        location: str = "dublin",
        property_type: str = "all",  # "all", "houses", "apartments"
        max_pages: int = 5
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ —Å –æ–±—Ö–æ–¥–æ–º –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        
        Args:
            min_bedrooms: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∞–ª–µ–Ω
            max_price: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –≤ –µ–≤—Ä–æ
            location: –õ–æ–∫–∞—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            property_type: –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ ("all", "houses", "apartments")
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ö–æ–¥–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
        """
        self.stats['start_time'] = datetime.datetime.now()
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫: {min_bedrooms}+ —Å–ø–∞–ª–µ–Ω, –¥–æ ‚Ç¨{max_price}, {location}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
        if location.lower() == 'dublin':
            if property_type == "houses":
                base_search_url = f"{self.base_url}/property-for-rent/dublin-city/houses?rentalPrice_to={max_price}&numBeds_from={min_bedrooms}&pageSize=20"
            elif property_type == "apartments":
                base_search_url = f"{self.base_url}/property-for-rent/dublin-city/apartments?rentalPrice_to={max_price}&numBeds_from={min_bedrooms}&pageSize=20"
            else:  # all
                base_search_url = f"{self.base_url}/property-for-rent/dublin-city?rentalPrice_to={max_price}&numBeds_from={min_bedrooms}&pageSize=20"
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç
            if property_type == "houses":
                base_search_url = f"{self.base_url}/property-for-rent/{location}/houses?rentalPrice_to={max_price}&numBeds_from={min_bedrooms}&pageSize=20"
            elif property_type == "apartments":
                base_search_url = f"{self.base_url}/property-for-rent/{location}/apartments?rentalPrice_to={max_price}&numBeds_from={min_bedrooms}&pageSize=20"
            else:  # all
                base_search_url = f"{self.base_url}/property-for-rent/{location}?rentalPrice_to={max_price}&numBeds_from={min_bedrooms}&pageSize=20"
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                all_property_urls = await self._collect_all_property_urls(page, base_search_url, max_pages)
                self.logger.info(f"üîó –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_property_urls)}")
                
                # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                results = []
                for i, url in enumerate(all_property_urls, 1):
                    self.logger.info(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {i}/{len(all_property_urls)}: {self._get_property_id(url)}")
                    
                    property_data = await self._parse_property_with_retry(page, url)
                    if property_data:
                        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                        if self._validate_property_data(property_data):
                            results.append(property_data)
                            self.stats['successful_parses'] += 1
                            self._log_property_summary(property_data)
                        else:
                            self.logger.warning(f"‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: {url}")
                            self.stats['failed_parses'] += 1
                    else:
                        self.stats['failed_parses'] += 1
                    
                    self.stats['total_processed'] += 1
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(0.5)
                
                self.stats['end_time'] = datetime.datetime.now()
                self._log_final_statistics()
                
                return results
                
            except Exception as e:
                self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
                return []
                
            finally:
                await browser.close()
    
    async def _collect_all_property_urls(self, page, base_url: str, max_pages: int) -> List[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        all_urls = set()
        current_page = 1
        
        while current_page <= max_pages:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if current_page == 1:
                page_url = base_url
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä from –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                separator = "&" if "?" in base_url else "?"
                page_url = f"{base_url}{separator}from={(current_page-1)*20}"
            
            self.logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {current_page}: {page_url}")
            
            try:
                await page.goto(page_url, wait_until='networkidle', timeout=self.page_timeout)
                await page.wait_for_timeout(2000)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                if current_page == 1:
                    total_count = await self._get_results_count(page)
                    self.logger.info(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {total_count}")
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                page_urls = await self._collect_property_urls_on_page(page)
                
                if not page_urls:
                    self.logger.info(f"üîö –ë–æ–ª—å—à–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page}")
                    break
                
                before_count = len(all_urls)
                all_urls.update(page_urls)
                new_urls = len(all_urls) - before_count
                
                self.logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}: –Ω–∞–π–¥–µ–Ω–æ {len(page_urls)} —Å—Å—ã–ª–æ–∫, –Ω–æ–≤—ã—Ö: {new_urls}")
                self.stats['total_pages'] += 1
                
                # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–µ–Ω—å—à–µ 20 –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                if len(page_urls) < 20:
                    self.logger.info(f"üîö –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {current_page}")
                    break
                
                current_page += 1
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_page}: {e}")
                break
        
        self.stats['total_links_found'] = len(all_urls)
        return list(all_urls)
    
    async def _collect_property_urls_on_page(self, page) -> List[str]:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            await page.wait_for_selector('a[href*="/for-rent/"]', timeout=10000)
            
            property_links = await page.query_selector_all('a[href*="/for-rent/"]')
            
            urls = []
            for link in property_links:
                href = await link.get_attribute('href')
                if href and '/for-rent/' in href:
                    # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞
                    if 'property-for-rent' not in href:
                        if href.startswith('/'):
                            href = self.base_url + href
                        urls.append(href)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_urls = list(set(urls))
            return unique_urls
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫: {e}")
            return []
    
    async def _get_results_count(self, page) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        try:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            selectors = [
                'h1',
                '[data-testid="results-count"]',
                '.SearchHeader__count'
            ]
            
            for selector in selectors:
                try:
                    element = await page.wait_for_selector(selector, timeout=5000)
                    text = await element.text_content()
                    if text:
                        count_match = re.search(r'(\d+)', text)
                        if count_match:
                            return int(count_match.group(1))
                except:
                    continue
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        
        return 0
    
    def _get_property_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL"""
        try:
            # –ò—â–µ–º ID –≤ –∫–æ–Ω—Ü–µ URL
            id_match = re.search(r'/(\d+)/?$', url)
            if id_match:
                return id_match.group(1)
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± - –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å URL
            return url.split('/')[-1] if url.split('/')[-1] else url.split('/')[-2]
        except:
            return "unknown"
    
    async def _parse_property_with_retry(self, page, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Å –ª–æ–≥–∏–∫–æ–π –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫"""
        for attempt in range(self.max_retries + 1):
            try:
                result = await self._parse_property(page, url)
                if result:
                    return result
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ—É–¥–∞—á–Ω–∞ –¥–ª—è {url}: {e}")
                
                if attempt < self.max_retries:
                    self.stats['retries'] += 1
                    await asyncio.sleep(self.retry_delay)
                else:
                    self.logger.error(f"‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è {url}")
        
        return None
    
    async def _parse_property(self, page, url: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            await page.goto(url, wait_until='domcontentloaded', timeout=self.property_timeout)
            await page.wait_for_timeout(1500)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_content = await page.content()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            property_data = {
                'url': url,
                'property_id': self._get_property_id(url),
                'title': await self._extract_title(page),
                'price': await self._extract_price(page, page_content),
                'bedrooms': await self._extract_bedrooms_improved(page, page_content),
                'bathrooms': await self._extract_bathrooms(page, page_content),
                'property_type': self._extract_property_type(page_content),
                'location': await self._extract_location(page),
                'description': self._extract_description(page_content),
                'features': await self._extract_features(page),
                'ber_rating': self._extract_ber_rating(page_content),
                'posted_date': self._extract_posted_date(page_content),
                'parsed_at': datetime.datetime.now().isoformat()
            }
            
            return property_data
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            raise
    
    async def _extract_title(self, page) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        selectors = [
            'h1',
            '[data-testid="title"]',
            '.TitleBlock__title'
        ]
        
        for selector in selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    title = await element.text_content()
                    if title and len(title.strip()) > 5:
                        return title.strip()
            except:
                continue
        
        return None
    
    async def _extract_price(self, page, page_content: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        # –ü—Ä–æ–±—É–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        price_selectors = [
            '[data-testid="price"]',
            '.TitleBlock__price',
            '.PropertyMainInfo__price',
            'span:has-text("‚Ç¨")'
        ]
        
        for selector in price_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    price_text = await element.text_content()
                    if price_text:
                        # –ò—â–µ–º —Ü–µ–Ω—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ ‚Ç¨1,234 –∏–ª–∏ ‚Ç¨1234
                        price_match = re.search(r'‚Ç¨\s*([\d,]+)', price_text)
                        if price_match:
                            price_str = price_match.group(1).replace(',', '')
                            price = int(price_str)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å —Ü–µ–Ω—ã (–æ—Ç ‚Ç¨100 –¥–æ ‚Ç¨10000)
                            if 100 <= price <= 10000:
                                return price
            except:
                continue
        
        # –ò—â–µ–º –≤ JSON –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            json_patterns = [
                r'"price":\s*(\d+)',
                r'"rentalPrice":\s*(\d+)',
                r'"monthlyRent":\s*(\d+)'
            ]
            
            for pattern in json_patterns:
                match = re.search(pattern, page_content)
                if match:
                    price = int(match.group(1))
                    if 100 <= price <= 10000:
                        return price
        except:
            pass
        
        return None
    
    async def _extract_bedrooms_improved(self, page, page_content: str) -> Optional[int]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ø–∞–ª–µ–Ω"""
        
        # 1. –ò—â–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö
        bed_selectors = [
            '[data-testid="bed-bath"]',
            '[data-testid="bedrooms"]', 
            '.PropertyDetailsList__item:has-text("bed")',
            '.TitleBlock__meta'
        ]
        
        for selector in bed_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    text = await element.text_content()
                    if text:
                        # –ò—â–µ–º "X bed" –∏–ª–∏ "X bedroom"
                        bed_match = re.search(r'(\d+)\s*(?:bed|bedroom)', text.lower())
                        if bed_match:
                            bedrooms = int(bed_match.group(1))
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å (–æ—Ç 1 –¥–æ 10 —Å–ø–∞–ª–µ–Ω)
                            if 1 <= bedrooms <= 10:
                                return bedrooms
            except:
                continue
        
        # 2. –ò—â–µ–º –≤ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        try:
            # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–ø–∞–ª–µ–Ω
            json_patterns = [
                r'"numberOfBedrooms":\s*(\d+)',
                r'"bedrooms":\s*(\d+)',
                r'"numBedrooms":\s*(\d+)',
                r'"bed":\s*(\d+)'
            ]
            
            for pattern in json_patterns:
                match = re.search(pattern, page_content)
                if match:
                    bedrooms = int(match.group(1))
                    if 1 <= bedrooms <= 10:
                        return bedrooms
        except:
            pass
        
        # 3. –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ –æ–ø–∏—Å–∞–Ω–∏—è (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—á–µ–Ω—å —è–≤–Ω–æ)
        try:
            # –ò—â–µ–º –æ—á–µ–Ω—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            description_patterns = [
                r'(\d+)\s*bedroom\s+(?:apartment|house|property)',
                r'this\s+(\d+)\s*bed',
                r'(\d+)\s*bed\s+(?:apartment|house|property)'
            ]
            
            for pattern in description_patterns:
                match = re.search(pattern, page_content.lower())
                if match:
                    bedrooms = int(match.group(1))
                    if 1 <= bedrooms <= 6:  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
                        return bedrooms
        except:
            pass
        
        # 4. –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å - –∏—â–µ–º –≤ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã—Ö
        try:
            meta_match = re.search(r'content="(\d+)\s*bedroom', page_content, re.IGNORECASE)
            if meta_match:
                bedrooms = int(meta_match.group(1))
                if 1 <= bedrooms <= 10:
                    return bedrooms
        except:
            pass
        
        return None
    
    async def _extract_bathrooms(self, page, page_content: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–Ω–Ω—ã—Ö –∫–æ–º–Ω–∞—Ç"""
        # –ò—â–µ–º –≤ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö
        bath_selectors = [
            '[data-testid="bed-bath"]',
            '[data-testid="bathrooms"]'
        ]
        
        for selector in bath_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    text = await element.text_content()
                    if text:
                        bath_match = re.search(r'(\d+)\s*(?:bath|bathroom)', text.lower())
                        if bath_match:
                            bathrooms = int(bath_match.group(1))
                            if 1 <= bathrooms <= 5:
                                return bathrooms
            except:
                continue
        
        # –ò—â–µ–º –≤ JSON
        try:
            bath_patterns = [
                r'"numberOfBathrooms":\s*(\d+)',
                r'"bathrooms":\s*(\d+)',
                r'"bath":\s*(\d+)'
            ]
            
            for pattern in bath_patterns:
                match = re.search(pattern, page_content)
                if match:
                    bathrooms = int(match.group(1))
                    if 1 <= bathrooms <= 5:
                        return bathrooms
        except:
            pass
        
        return None
    
    async def _extract_location(self, page) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ"""
        try:
            # –ò—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            title_element = await page.query_selector('h1')
            if title_element:
                title = await title_element.text_content()
                if title:
                    # –ò—â–µ–º Dublin —Å –Ω–æ–º–µ—Ä–æ–º –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ–º
                    location_patterns = [
                        r'Dublin\s+\d+',
                        r'Dublin\s+\w+',
                        r'Co\.\s*Dublin'
                    ]
                    
                    for pattern in location_patterns:
                        match = re.search(pattern, title, re.IGNORECASE)
                        if match:
                            return match.group()
        except:
            pass
        
        return None
    
    def _extract_property_type(self, page_content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        try:
            # –ò—â–µ–º –≤ JSON –¥–∞–Ω–Ω—ã—Ö
            type_patterns = [
                r'"propertyType":\s*"([^"]*)"',
                r'"@type":\s*"([^"]*)"',
                r'"category":\s*"([^"]*)"'
            ]
            
            for pattern in type_patterns:
                match = re.search(pattern, page_content)
                if match:
                    prop_type = match.group(1)
                    if prop_type and prop_type.lower() not in ['breadcrumblist', 'webpage']:
                        return prop_type
            
            # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
            if 'apartment' in page_content.lower():
                return 'Apartment'
            elif 'house' in page_content.lower():
                return 'House'
            elif 'studio' in page_content.lower():
                return 'Studio'
                
        except:
            pass
        
        return None
    
    def _extract_description(self, page_content: str, max_length: int = 300) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            # –ò—â–µ–º –≤ JSON –¥–∞–Ω–Ω—ã—Ö
            desc_patterns = [
                r'"description":\s*"([^"]*)"',
                r'"propertyDescription":\s*"([^"]*)"'
            ]
            
            for pattern in desc_patterns:
                match = re.search(pattern, page_content)
                if match:
                    description = match.group(1)
                    if description and len(description) > 20:
                        if len(description) > max_length:
                            description = description[:max_length] + "..."
                        return description
        except:
            pass
        
        return None
    
    async def _extract_features(self, page) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        features = []
        
        try:
            # –ò—â–µ–º —Å–ø–∏—Å–∫–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π
            feature_selectors = [
                '.PropertyDetailsList__item',
                '.FeaturesList__item',
                '[data-testid="features"] li'
            ]
            
            for selector in feature_selectors:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    text = await element.text_content()
                    if text and len(text.strip()) > 2:
                        features.append(text.strip())
                        
                if features:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                    break
                    
        except:
            pass
        
        return features[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π
    
    def _extract_ber_rating(self, page_content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç BER —Ä–µ–π—Ç–∏–Ω–≥ —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        try:
            ber_patterns = [
                r'"ber":\s*"([^"]*)"',
                r'"berRating":\s*"([^"]*)"',
                r'BER[:\s]+([A-G]\d*)',
                r'Energy Rating[:\s]+([A-G]\d*)'
            ]
            
            for pattern in ber_patterns:
                match = re.search(pattern, page_content, re.IGNORECASE)
                if match:
                    ber = match.group(1).strip()
                    if ber and len(ber) <= 3:
                        return ber
        except:
            pass
        
        return None
    
    def _extract_posted_date(self, page_content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            date_patterns = [
                r'"datePublished":\s*"([^"]*)"',
                r'"postedDate":\s*"([^"]*)"',
                r'"listedDate":\s*"([^"]*)"'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_content)
                if match:
                    return match.group(1)
        except:
            pass
        
        return None
    
    def _validate_property_data(self, data: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if not data.get('title') and not data.get('price'):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å —Ü–µ–Ω—ã
        price = data.get('price')
        if price and (price < 100 or price > 10000):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ø–∞–ª–µ–Ω
        bedrooms = data.get('bedrooms')
        if bedrooms and (bedrooms < 1 or bedrooms > 10):
            return False
        
        return True
    
    def _log_property_summary(self, prop: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
        title = prop.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:40]
        price = f"‚Ç¨{prop['price']}" if prop.get('price') else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
        bedrooms = f"{prop['bedrooms']} —Å–ø–∞–ª–µ–Ω" if prop.get('bedrooms') else '–°–ø–∞–ª—å–Ω–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã'
        location = prop.get('location', '–õ–æ–∫–∞—Ü–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞')
        
        self.logger.info(f"‚úÖ {title} | {price} | {bedrooms} | {location}")
    
    def _log_final_statistics(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        self.logger.info("=" * 60)
        self.logger.info("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        self.logger.info("=" * 60)
        self.logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.1f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self.stats['total_pages']}")
        self.logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {self.stats['total_links_found']}")
        self.logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {self.stats['total_processed']}")
        self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {self.stats['successful_parses']}")
        self.logger.info(f"‚ùå –ù–µ—É–¥–∞—á–Ω–æ: {self.stats['failed_parses']}")
        self.logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫: {self.stats['retries']}")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful_parses'] / self.stats['total_processed']) * 100
            self.logger.info(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
    
    def save_results(self, results: List[Dict[str, Any]], search_params: Dict[str, Any]) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.results_dir / f"daft_results_{timestamp}.json"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º datetime –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON
        stats_copy = self.stats.copy()
        if stats_copy['start_time']:
            stats_copy['start_time'] = stats_copy['start_time'].isoformat()
        if stats_copy['end_time']:
            stats_copy['end_time'] = stats_copy['end_time'].isoformat()
        
        output_data = {
            'search_params': search_params,
            'statistics': stats_copy,
            'results_count': len(results),
            'results': results,
            'generated_at': datetime.datetime.now().isoformat()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        return str(filename)
    
    def format_results_summary(self, results: List[Dict[str, Any]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not results:
            return "‚ùå –û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–Ω–∞–º
        prices = [r['price'] for r in results if r.get('price')]
        bedrooms = [r['bedrooms'] for r in results if r.get('bedrooms')]
        
        summary = [
            f"üè† –ù–ê–ô–î–ï–ù–û {len(results)} –û–ë–™–Ø–í–õ–ï–ù–ò–ô",
            "",
            "üí∞ –¶–ï–ù–´:",
        ]
        
        if prices:
            summary.extend([
                f"   –°—Ä–µ–¥–Ω—è—è: ‚Ç¨{sum(prices) / len(prices):.0f}",
                f"   –î–∏–∞–ø–∞–∑–æ–Ω: ‚Ç¨{min(prices)} - ‚Ç¨{max(prices)}"
            ])
        
        if bedrooms:
            summary.extend([
                "",
                "üõèÔ∏è –°–ü–ê–õ–¨–ù–ò:",
                f"   –°—Ä–µ–¥–Ω–µ–µ: {sum(bedrooms) / len(bedrooms):.1f}",
                f"   –î–∏–∞–ø–∞–∑–æ–Ω: {min(bedrooms)} - {max(bedrooms)}"
            ])
        
        # –¢–æ–ø-5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        summary.extend([
            "",
            "üîù –¢–û–ü-5 –û–ë–™–Ø–í–õ–ï–ù–ò–ô:",
            ""
        ])
        
        for i, prop in enumerate(results[:5], 1):
            title = prop.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
            price = f"‚Ç¨{prop['price']}" if prop.get('price') else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
            beds = f"{prop['bedrooms']} —Å–ø–∞–ª–µ–Ω" if prop.get('bedrooms') else '–°–ø–∞–ª—å–Ω–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã'
            
            summary.append(f"{i}. {title}")
            summary.append(f"   üí∞ {price} | üõèÔ∏è {beds}")
            summary.append("")
        
        return "\n".join(summary)

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("üöÄ PRODUCTION DAFT.IE PARSER - –ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø")
    print("=" * 60)
    
    parser = ProductionDaftParser(log_level="INFO")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
    search_params = {
        'min_bedrooms': 3,
        'max_price': 2500,
        'location': 'dublin',
        'property_type': 'houses',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º houses –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        'max_pages': 10  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞
    }
    
    print("üéØ –ü–ê–†–ê–ú–ï–¢–†–´ –ü–û–ò–°–ö–ê:")
    for key, value in search_params.items():
        print(f"   {key}: {value}")
    print()
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
    results = await parser.search_all_properties(**search_params)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 60)
    print("üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 60)
    
    summary = parser.format_results_summary(results)
    print(summary)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    filename = parser.save_results(results, search_params)
    print(f"\nüíæ –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

if __name__ == "__main__":
    asyncio.run(main())
