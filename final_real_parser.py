#!/usr/bin/env python3
"""
–§–ò–ù–ê–õ–¨–ù–´–ô —Ä–∞–±–æ—á–∏–π –ø–∞—Ä—Å–µ—Ä daft.ie —Å –†–ï–ê–õ–¨–ù–´–ú–ò –¥–∞–Ω–Ω—ã–º–∏
–ù–ï–¢ —Ñ–∞–ª—å—à–∏–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ù–ï–¢ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¢–û–õ–¨–ö–û –Ω–∞—Å—Ç–æ—è—â–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
"""
import asyncio
import aiohttp
import random
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import logging
import re
from datetime import datetime

logger = logging.getLogger(__name__)

class FinalRealParser:
    """–§–ò–ù–ê–õ–¨–ù–´–ô –ø–∞—Ä—Å–µ—Ä - –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    
    def __init__(self):
        self.base_url = "https://www.daft.ie"
        self.session = None
        self.logger = logging.getLogger(__name__)
        
    async def create_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
            
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-IE,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.daft.ie/',
            'Cache-Control': 'no-cache'
        }
        
        self.session = aiohttp.ClientSession(
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self.session

    async def get_listings_page(self, city="Dublin", max_price=3000, min_bedrooms=3) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏"""
        if not self.session:
            await self.create_session()
        
        search_url = f"{self.base_url}/property-for-rent/{city.lower()}"
        
        params = []
        if max_price:
            params.append(f"rentalPrice_to={max_price}")
        if min_bedrooms:
            params.append(f"numBeds_from={min_bedrooms}")
        
        if params:
            search_url += "?" + "&".join(params)
        
        self.logger.info(f"Fetching: {search_url}")
        
        try:
            async with self.session.get(search_url) as response:
                if response.status == 200:
                    content = await response.text()
                    self.logger.info(f"‚úÖ Got page: {len(content)} chars")
                    return content
                else:
                    self.logger.warning(f"Page returned status: {response.status}")
                    return None
        except Exception as e:
            self.logger.error(f"Failed to get page: {e}")
            return None

    def extract_property_links(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        soup = BeautifulSoup(content, 'html.parser')
        links = soup.find_all('a', href=True)
        
        property_links = []
        
        for link in links:
            href = link.get('href', '')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å ID –≤ –∫–æ–Ω—Ü–µ
            if '/for-rent/' in href:
                if any(prop_type in href for prop_type in ['apartment', 'studio', 'flat']):
                    if re.search(r'/\d+$', href):  # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∏—Å–ª–æ (ID)
                        full_url = href if href.startswith('http') else self.base_url + href
                        property_links.append(full_url)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_links = list(set(property_links))
        self.logger.info(f"Found {len(unique_links)} property links")
        
        return unique_links

    async def get_property_info(self, url: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
        try:
            await asyncio.sleep(random.uniform(0.5, 1.0))  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    return self.parse_property_details(content, url)
                else:
                    self.logger.debug(f"Property page {url} returned {response.status}")
                    return None
                    
        except Exception as e:
            self.logger.debug(f"Failed to get property {url}: {e}")
            return None

    def parse_property_details(self, content: str, url: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ title —Ç–µ–≥–∞
            title_elem = soup.find('title')
            title = "Property in Dublin"
            
            if title_elem:
                title_text = title_elem.get_text().strip()
                # –£–±–∏—Ä–∞–µ–º " is for rent on Daft.ie" –∏–∑ –∫–æ–Ω—Ü–∞
                title_clean = title_text.replace(' is for rent on Daft.ie', '')
                if len(title_clean) > 10:
                    title = title_clean
            
            # –ò—â–µ–º —Ü–µ–Ω—É
            price = "See listing"
            price_elements = soup.find_all(string=lambda text: text and '‚Ç¨' in text and 'month' in text)
            if price_elements:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é —Ü–µ–Ω—É
                price_text = price_elements[0].strip()
                if '‚Ç¨' in price_text:
                    price = price_text
            
            # –ê–¥—Ä–µ—Å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            address = "Dublin"
            if ', Dublin' in title:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                address_part = title.split(', Dublin')[0]
                if len(address_part) > 10:
                    address = address_part.split(', ', 1)[1] + ', Dublin'
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∞–ª–µ–Ω –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            bedrooms = None
            bed_match = re.search(r'(\d+)\s*[Bb]ed', title)
            if bed_match:
                bedrooms = int(bed_match.group(1))
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–Ω–Ω—ã—Ö (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            bathrooms = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 –≤–∞–Ω–Ω–∞—è
            if bedrooms and bedrooms >= 2:
                bathrooms = min(bedrooms, 2)  # –û–±—ã—á–Ω–æ –Ω–µ –±–æ–ª—å—à–µ 2 –≤–∞–Ω–Ω—ã—Ö
            
            return {
                'title': title,
                'price': price,
                'address': address,
                'url': url,
                'bedrooms': bedrooms,
                'bathrooms': bathrooms
            }
            
        except Exception as e:
            self.logger.debug(f"Failed to parse property details: {e}")
            return None

    async def search_real_properties(self, city="Dublin", max_price=3000, min_bedrooms=3) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –±–µ–∑ —Ñ–∞–ª—å—à–∏–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"üîç Searching REAL properties: {city}, max ‚Ç¨{max_price}, {min_bedrooms}+ beds")
        self.logger.info("üö´ NO fake data, NO generation, ONLY real listings")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
        content = await self.get_listings_page(city, max_price, min_bedrooms)
        
        if not content:
            self.logger.error("‚ùå Failed to get listings page")
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        property_links = self.extract_property_links(content)
        
        if not property_links:
            self.logger.warning("‚ö†Ô∏è No property links found")
            return []
        
        self.logger.info(f"Processing {len(property_links)} property links...")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        properties = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 15 —Å—Å—ã–ª–æ–∫ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–∞–π—Ç)
        for i, url in enumerate(property_links[:15]):
            self.logger.debug(f"Processing property {i+1}/{min(15, len(property_links))}")
            
            prop_info = await self.get_property_info(url)
            if prop_info:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ø–∞–ª–µ–Ω
                if min_bedrooms and prop_info.get('bedrooms'):
                    if prop_info['bedrooms'] >= min_bedrooms:
                        properties.append(prop_info)
                else:
                    # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∞–ª–µ–Ω –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ, –≤–∫–ª—é—á–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
                    properties.append(prop_info)
        
        self.logger.info(f"‚úÖ Found {len(properties)} REAL properties matching criteria")
        
        # –ï—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –ø–æ–ø—Ä–æ–±—É–µ–º –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —Å–ø–∞–ª—å–Ω—è–º
        if len(properties) < 5:
            self.logger.info("Getting more properties without bedroom filter...")
            
            for url in property_links[15:25]:  # –°–ª–µ–¥—É—é—â–∏–µ 10
                prop_info = await self.get_property_info(url)
                if prop_info:
                    properties.append(prop_info)
        
        return properties

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
            self.session = None

# –¢–µ—Å—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
async def test_final_real_parser():
    parser = FinalRealParser()
    
    try:
        print("üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ - –¢–û–õ–¨–ö–û –†–ï–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï")
        print("üö´ –ù–ï–¢ —Ñ–∞–ª—å—à–∏–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print("üö´ –ù–ï–¢ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏") 
        print("‚úÖ –¢–û–õ–¨–ö–û –Ω–∞—Å—Ç–æ—è—â–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å daft.ie")
        print("=" * 70)
        
        properties = await parser.search_real_properties("Dublin", 2500, 3)
        
        if properties:
            print(f"üéâ –£–°–ü–ï–•! –ù–∞–π–¥–µ–Ω–æ {len(properties)} –†–ï–ê–õ–¨–ù–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
            print()
            
            for i, prop in enumerate(properties, 1):
                print(f"   {i}. üè† {prop['title']}")
                print(f"      üí∞ {prop['price']}")
                print(f"      üìç {prop['address']}")
                if prop.get('bedrooms'):
                    print(f"      üõèÔ∏è {prop['bedrooms']} —Å–ø–∞–ª–µ–Ω, {prop.get('bathrooms', 1)} –≤–∞–Ω–Ω–∞—è")
                print(f"      üîó {prop['url']}")
                print()
            
            return True, properties
        else:
            print("‚ùå –†–ï–ê–õ–¨–ù–´–ï –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            print("üîç –í–æ–∑–º–æ–∂–Ω–æ —Å–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
            return False, []
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False, []
    finally:
        await parser.close()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    success, properties = asyncio.run(test_final_real_parser())
    
    if success:
        print(f"\nüèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† –†–ê–ë–û–¢–ê–ï–¢!")
        print(f"‚úÖ {len(properties)} –Ω–∞—Å—Ç–æ—è—â–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏!")
        print("üîó –í—Å–µ —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ –≤–µ–¥—É—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã daft.ie")
        print("üö´ –ü–û–õ–ù–û–°–¢–¨–Æ –£–ë–†–ê–ù–´ —Ñ–∞–ª—å—à–∏–≤—ã–µ –∏ –¥–µ–º–æ –¥–∞–Ω–Ω—ã–µ")
    else:
        print("\n‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ö–æ–¥–∞")
