#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã daft.ie –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import re

async def detailed_html_analysis():
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ HTML –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
    
    url = "https://www.daft.ie/property-for-rent/dublin?rentalPrice_to=2500&numBeds_from=3"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    print(f"üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
    print(f"URL: {url}")
    print("="*80)
    
    async with aiohttp.ClientSession(headers=headers) as session:
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    print(f"üìÑ –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(content):,} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # 1. –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
                    all_links = soup.find_all('a')
                    print(f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_links)}")
                    
                    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
                    for_rent_links = [link for link in all_links if link.get('href') and 'for-rent' in link.get('href')]
                    print(f"üè† –°—Å—ã–ª–æ–∫ —Å 'for-rent': {len(for_rent_links)}")
                    
                    # 3. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
                    print(f"\nüìã –ü–µ—Ä–≤—ã–µ 10 —Å—Å—ã–ª–æ–∫ —Å 'for-rent':")
                    for i, link in enumerate(for_rent_links[:10]):
                        href = link.get('href')
                        text = link.get_text().strip()[:50]
                        print(f"  {i+1}. {href} | {text}...")
                    
                    # 4. –ò—â–µ–º –¥—Ä—É–≥–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                    print(f"\nüîç –ü–æ–∏—Å–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤:")
                    
                    # –ò—â–µ–º –ø–æ data –∞—Ç—Ä–∏–±—É—Ç–∞–º
                    data_elements = soup.find_all(attrs={"data-testid": True})
                    print(f"üìä –≠–ª–µ–º–µ–Ω—Ç–æ–≤ —Å data-testid: {len(data_elements)}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ data-testid
                    testids = set([elem.get('data-testid') for elem in data_elements[:20]])
                    for testid in sorted(testids):
                        print(f"  ‚Ä¢ data-testid='{testid}'")
                    
                    # 5. –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
                    elements_with_class = soup.find_all(class_=True)
                    classes = set()
                    for elem in elements_with_class[:50]:
                        if elem.get('class'):
                            classes.update(elem.get('class'))
                    
                    print(f"\nüé® –ß–∞—Å—Ç—ã–µ –∫–ª–∞—Å—Å—ã CSS:")
                    common_classes = [cls for cls in classes if any(word in cls.lower() for word in ['card', 'item', 'property', 'listing'])]
                    for cls in sorted(common_classes)[:10]:
                        print(f"  ‚Ä¢ {cls}")
                    
                    # 6. –ò—â–µ–º JSON –¥–∞–Ω–Ω—ã–µ
                    scripts = soup.find_all('script')
                    print(f"\nüìú Script —Ç–µ–≥–æ–≤: {len(scripts)}")
                    
                    for i, script in enumerate(scripts):
                        if script.string and ('property' in script.string.lower() or 'listing' in script.string.lower()):
                            content_preview = script.string[:200].replace('\n', ' ')
                            print(f"  Script {i}: {content_preview}...")
                    
                    # 7. –ü–æ–∏—Å–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    page_text = soup.get_text()
                    result_indicators = []
                    
                    lines = page_text.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line and ('result' in line.lower() or 'found' in line.lower() or 'property' in line.lower()):
                            if any(char.isdigit() for char in line) and len(line) < 100:
                                result_indicators.append(line)
                    
                    print(f"\nüìà –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
                    for indicator in result_indicators[:5]:
                        print(f"  ‚Ä¢ {indicator}")
                        
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞: {response.status}")
                    
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

if __name__ == "__main__":
    asyncio.run(detailed_html_analysis())
