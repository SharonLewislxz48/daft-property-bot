#!/usr/bin/env python3
"""
–û–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ - –º–µ—Ç–æ–¥ 3: –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ –∏ TOR
"""
import asyncio
import aiohttp
import random
import json
from typing import List, Dict, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProxyDaftParser:
    """–ü–∞—Ä—Å–µ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∫—Å–∏"""
    
    def __init__(self):
        self.base_url = "https://www.daft.ie"
        self.session = None
        
        # –°–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        self.free_proxies = []
        
    async def get_free_proxies(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏"""
        proxy_sources = [
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-http.txt"
        ]
        
        proxies = []
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∫—Å–∏
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
            for source in proxy_sources:
                try:
                    logger.info(f"Fetching proxies from: {source}")
                    async with session.get(source) as response:
                        if response.status == 200:
                            text = await response.text()
                            lines = text.strip().split('\n')
                            
                            for line in lines:
                                line = line.strip()
                                if ':' in line and len(line.split(':')) == 2:
                                    ip, port = line.split(':')
                                    if self.is_valid_ip(ip) and port.isdigit():
                                        proxies.append(f"http://{line}")
                                        
                except Exception as e:
                    logger.debug(f"Failed to fetch from {source}: {e}")
                    continue
        
        logger.info(f"Collected {len(proxies)} proxy addresses")
        return proxies[:50]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 50 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    
    def is_valid_ip(self, ip: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ IP –∞–¥—Ä–µ—Å–∞"""
        try:
            parts = ip.split('.')
            return len(parts) == 4 and all(0 <= int(part) <= 255 for part in parts)
        except:
            return False
    
    async def test_proxy(self, proxy: str) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–æ–∫—Å–∏"""
        try:
            connector = aiohttp.TCPConnector(limit=1)
            timeout = aiohttp.ClientTimeout(total=10, connect=5)
            
            async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                trust_env=False
            ) as session:
                
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º —Å–∞–π—Ç–µ
                test_url = "http://httpbin.org/ip"
                
                async with session.get(test_url, proxy=proxy) as response:
                    if response.status == 200:
                        data = await response.json()
                        logger.info(f"‚úÖ Proxy works: {proxy} -> IP: {data.get('origin')}")
                        return True
                        
        except Exception as e:
            logger.debug(f"‚ùå Proxy failed: {proxy} - {e}")
            return False
        
        return False
    
    async def find_working_proxies(self, max_proxies: int = 5) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –ø—Ä–æ–∫—Å–∏"""
        logger.info("üîç Searching for working proxies...")
        
        all_proxies = await self.get_free_proxies()
        if not all_proxies:
            logger.error("No proxies found")
            return []
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
        random.shuffle(all_proxies)
        
        working_proxies = []
        tasks = []
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∫—Å–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        for proxy in all_proxies[:20]:  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 20
            tasks.append(self.test_proxy(proxy))
        
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if result is True and len(working_proxies) < max_proxies:
                    working_proxies.append(all_proxies[i])
        
        logger.info(f"Found {len(working_proxies)} working proxies")
        return working_proxies
    
    async def create_proxy_session(self, proxy: str = None):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–æ–∫—Å–∏"""
        if self.session:
            await self.session.close()
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        connector = aiohttp.TCPConnector(
            limit=5,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        
        session_kwargs = {
            'headers': headers,
            'connector': connector,
            'timeout': timeout,
            'trust_env': False
        }
        
        self.session = aiohttp.ClientSession(**session_kwargs)
        logger.info(f"Session created with proxy: {proxy or 'None'}")
        return self.session
    
    async def try_tor_connection(self) -> bool:
        """–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ TOR"""
        try:
            # TOR –æ–±—ã—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø–æ—Ä—Ç—É 9050 (SOCKS) –∏–ª–∏ 8118 (HTTP)
            tor_proxies = [
                "socks5://127.0.0.1:9050",
                "http://127.0.0.1:8118",
                "socks5://localhost:9050"
            ]
            
            for tor_proxy in tor_proxies:
                try:
                    logger.info(f"Trying TOR proxy: {tor_proxy}")
                    
                    connector = aiohttp.TCPConnector()
                    timeout = aiohttp.ClientTimeout(total=15)
                    
                    async with aiohttp.ClientSession(
                        connector=connector,
                        timeout=timeout,
                        trust_env=False
                    ) as session:
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º IP —á–µ—Ä–µ–∑ TOR
                        async with session.get("http://httpbin.org/ip", proxy=tor_proxy) as response:
                            if response.status == 200:
                                data = await response.json()
                                logger.info(f"‚úÖ TOR works! IP: {data.get('origin')}")
                                return True
                                
                except Exception as e:
                    logger.debug(f"TOR proxy {tor_proxy} failed: {e}")
                    continue
            
            logger.warning("TOR is not available")
            return False
            
        except Exception as e:
            logger.error(f"TOR connection error: {e}")
            return False
    
    async def fetch_with_proxy(self, url: str, proxy: str = None) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏"""
        session = await self.create_proxy_session()
        
        try:
            await asyncio.sleep(random.uniform(1, 3))
            
            kwargs = {}
            if proxy:
                kwargs['proxy'] = proxy
                logger.info(f"Fetching {url} via proxy: {proxy}")
            else:
                logger.info(f"Fetching {url} direct")
            
            async with session.get(url, **kwargs) as response:
                logger.info(f"Response: {response.status}")
                
                if response.status == 200:
                    content = await response.text()
                    
                    if len(content) > 10000 and 'daft.ie' in content:
                        logger.info(f"‚úÖ Success via proxy! Content: {len(content)} chars")
                        return content
                    else:
                        logger.warning(f"Content seems incomplete: {len(content)} chars")
                else:
                    logger.warning(f"HTTP {response.status}")
                    
        except Exception as e:
            logger.error(f"Proxy fetch error: {e}")
            
        return None
    
    async def search_with_proxies(self, city="Dublin", max_price=3000, min_bedrooms=2) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏"""
        logger.info(f"üîç Proxy search: {city}, max ‚Ç¨{max_price}, {min_bedrooms}+ beds")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL
        search_url = f"{self.base_url}/property-for-rent/{city.lower()}"
        params = []
        
        if max_price:
            params.append(f"rentalPrice_to={max_price}")
        if min_bedrooms:
            params.append(f"numBeds_from={min_bedrooms}")
        
        if params:
            search_url += "?" + "&".join(params)
        
        # –ú–µ—Ç–æ–¥ 1: –ü—Ä–æ–±—É–µ–º TOR
        if await self.try_tor_connection():
            logger.info("Trying with TOR...")
            html = await self.fetch_with_proxy(search_url, "socks5://127.0.0.1:9050")
            if html:
                return self.parse_html_for_properties(html)
        
        # –ú–µ—Ç–æ–¥ 2: –ü—Ä–æ–±—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–æ–∫—Å–∏
        working_proxies = await self.find_working_proxies()
        
        for proxy in working_proxies:
            logger.info(f"Trying proxy: {proxy}")
            html = await self.fetch_with_proxy(search_url, proxy)
            
            if html:
                properties = self.parse_html_for_properties(html)
                if properties:
                    logger.info(f"‚úÖ Success with proxy {proxy}!")
                    return properties
            
            await asyncio.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
        
        # –ú–µ—Ç–æ–¥ 3: –ü—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å
        logger.info("Trying direct connection...")
        html = await self.fetch_with_proxy(search_url)
        if html:
            return self.parse_html_for_properties(html)
        
        return []
    
    def parse_html_for_properties(self, html: str) -> List[Dict]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML"""
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        properties = []
        
        # –ò—â–µ–º –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link['href']
            if 'for-rent' in href and 'property' in href:
                title = link.get_text(strip=True)
                if title and len(title) > 10:
                    prop = {
                        'title': title,
                        'url': f"{self.base_url}{href}" if href.startswith('/') else href,
                        'address': 'Dublin',
                        'price': 'See listing'
                    }
                    properties.append(prop)
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ —Å–∫—Ä–∏–ø—Ç–∞—Ö
        scripts = soup.find_all('script')
        for script in scripts:
            content = script.string or ""
            if 'property' in content.lower() and '{' in content:
                try:
                    # –ò—â–µ–º JSON-–ø–æ–¥–æ–±–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                    import re
                    json_matches = re.findall(r'\{[^{}]*"title"[^{}]*\}', content)
                    for match in json_matches:
                        try:
                            data = json.loads(match)
                            if isinstance(data, dict) and 'title' in data:
                                prop = {
                                    'title': data.get('title', 'Property'),
                                    'address': data.get('address', 'Dublin'),
                                    'price': data.get('price', 'See listing'),
                                    'url': f"{self.base_url}/property"
                                }
                                properties.append(prop)
                        except:
                            continue
                except:
                    continue
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_properties = []
        seen_titles = set()
        
        for prop in properties:
            if prop['title'] not in seen_titles:
                seen_titles.add(prop['title'])
                unique_properties.append(prop)
        
        logger.info(f"Parsed {len(unique_properties)} unique properties")
        return unique_properties
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()

# –¢–µ—Å—Ç –ø—Ä–æ–∫—Å–∏ –ø–∞—Ä—Å–µ—Ä–∞
async def test_proxy_parser():
    parser = ProxyDaftParser()
    
    try:
        print("üåê –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ö–æ–¥ —á–µ—Ä–µ–∑ –ü–†–û–ö–°–ò –∏ TOR...")
        print("=" * 60)
        
        properties = await parser.search_with_proxies("Dublin", 3000, 2)
        
        if properties:
            print(f"‚úÖ –£–°–ü–ï–•! –ù–∞–π–¥–µ–Ω–æ {len(properties)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏:")
            print()
            
            for i, prop in enumerate(properties[:5], 1):
                print(f"   {i}. üè† {prop['title']}")
                print(f"      üìç {prop['address']}")
                print(f"      üí∞ {prop['price']}")
                print(f"      üîó {prop['url'][:80]}...")
                print()
            
            return True
        else:
            print("‚ùå –ü—Ä–æ–∫—Å–∏ –º–µ—Ç–æ–¥—ã –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        await parser.close()

if __name__ == "__main__":
    success = asyncio.run(test_proxy_parser())
    
    if success:
        print("\nüéâ –ú–ï–¢–û–î 3 –£–°–ü–ï–®–ï–ù! –ü—Ä–æ–∫—Å–∏ –æ–±—Ö–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    else:
        print("\n‚ö†Ô∏è –ú–µ—Ç–æ–¥ 3 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –º–µ—Ç–æ–¥—É 4...")
